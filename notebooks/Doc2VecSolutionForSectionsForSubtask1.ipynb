{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "cache_dir = \"./data_cache_doc2vec_solution_notebook\"\n",
    "\n",
    "dialogue_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "document_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"document_domain\",\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "rc_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"doc2dial_rc\",\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a model for each document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "document_full_df = pd.DataFrame(data=document_dataset)\n",
    "\n",
    "# key=span_id, value=text\n",
    "def span_dict_for_doc(doc_id):\n",
    "    #use pandas as way faster\n",
    "    document = document_full_df.loc[document_full_df['doc_id'] == doc_id]\n",
    "    spans_dict = {}\n",
    "    for span in document['spans'].iloc[0]:\n",
    "        spans_dict[span['id_sp']]=span['text_sp']\n",
    "\n",
    "    return spans_dict\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['Buy or sell a vehicle (transfer ownership)#3_0',\n 'Direct Deposit | Social Security Administration#2_0',\n 'Benefits Planner: Retirement | Benefits By Year Of Birth | Social Security Administration#1_0',\n 'Veterans Vocational Rehabilitation Programs | Veterans Affairs#1_0',\n 'Standard Plan | Federal Student Aid#1_0']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "#Getting a list of spans per grounding document !!!! Language confusion: spans are the documents in gensim land and the list of spans (aka the whole document) is the corpora in gensim\n",
    "\n",
    "#I only want to use documents from the document datasets that have dialogues (which not all have)\n",
    "unique_doc_ids = list(set(dialogue_dataset['doc_id']))\n",
    "\n",
    "#extract all the span texts for that doc id,\n",
    "#!!!Index drama. For list index starts with 0, for spans index starts with 1, use dict to keep the sp_id and doc_id\n",
    "#key=doc_id, value=span dict\n",
    "\n",
    "#TODO consider using text-sec instead of spans\n",
    "\n",
    "def text_sec_doc(doc_id):\n",
    "    #use pandas as way faster\n",
    "    document = document_full_df.loc[document_full_df['doc_id'] == doc_id]\n",
    "    #TODO interestingly the spans also have a text_sect which is split into less small segments and probably better\n",
    "    text_sec = []\n",
    "    current_text_sec = ''\n",
    "\n",
    "    for span in document['spans'].iloc[0]:\n",
    "        text = span['text_sec']\n",
    "        if current_text_sec != text: #found new sect\n",
    "            text_sec.append(text)\n",
    "            current_text_sec = text\n",
    "    return text_sec\n",
    "\n",
    "raw_training_text_per_doc = {}\n",
    "for doc_id in unique_doc_ids:\n",
    "    # doc_spans[doc_id] = span_dict_for_doc(doc_id) #for spans\n",
    "    raw_training_text_per_doc[doc_id] = text_sec_doc(doc_id) #for text_sec\n",
    "\n",
    "tokenized_training_docs = {}\n",
    "for doc_id in raw_training_text_per_doc:\n",
    "    text_sections = raw_training_text_per_doc[doc_id]\n",
    "    #simplest preprocessing from gemsim\n",
    "    tokenized_sections = [list(simple_preprocess(sec, deacc=True)) for sec in text_sections]\n",
    "    tokenized_training_docs[doc_id] = tokenized_sections\n",
    "\n",
    "list(tokenized_training_docs.keys())[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "415"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "models_for_doc = {}\n",
    "for doc_id in tokenized_training_docs:\n",
    "    tokenized_sec = tokenized_training_docs[doc_id]\n",
    "    training_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_sec)]\n",
    "    # Check how to fine tune the model https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    model = Doc2Vec(training_docs, vector_size=10, window=4, min_count=1, workers=4, epochs=30)\n",
    "    models_for_doc[doc_id] = model\n",
    "\n",
    "len(models_for_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict rc questions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': '9f44c1539efe6f7e79b02eb1b413aa43_1',\n  'prediction_text': 'States communicate with each other , so when you move to another state, be sure to tie up any loose ends regarding your New York State license or registration. That means resolving any unanswered tickets, suspensions or revocations, and surrendering your license plates to NYS when you get to your new home state. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_3',\n  'prediction_text': 'DMV maintains a point system to track dangerous drivers. Often , motorists convicted of a traffic ticket feel they have resolved all their motoring issues with the local court, but later learn that the Driver Responsibility Assessment DRA is a separate DMV charge based on the total points they accumulate. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_5',\n  'prediction_text': 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. Better yet , don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_7',\n  'prediction_text': 'DMV maintains a point system to track dangerous drivers. Often , motorists convicted of a traffic ticket feel they have resolved all their motoring issues with the local court, but later learn that the Driver Responsibility Assessment DRA is a separate DMV charge based on the total points they accumulate. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_9',\n  'prediction_text': 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. Better yet , don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. ',\n  'no_answer_probability': 0.0}]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for example in rc_dataset:\n",
    "    question_ = example[\"question\"]\n",
    "    doc_id =example['title']\n",
    "    # it does better if the user and agent string are left\n",
    "    # question_ = question_.replace('user:', '')\n",
    "    # question_ = question_.replace('agent:', '')\n",
    "\n",
    "    #preprocess question in the same way\n",
    "    test_doc = simple_preprocess(question_, deacc=True)\n",
    "    #calculate vector using model for that document\n",
    "    model = models_for_doc[doc_id]\n",
    "    vector = model.infer_vector(test_doc)\n",
    "    #find the most similar document (spans)\n",
    "    sims = model.dv.most_similar([vector], topn=1)\n",
    "    most_likely_answer = raw_training_text_per_doc[doc_id][sims[0][0]]\n",
    "    # most_likely_answer = training_docs[sims[0][0] - 1]\n",
    "    # most_likely_predicted_tag = most_likely_answer.tags\n",
    "    #find original text for tag\n",
    "    # most_likely_predicted_text = spans[most_likely_predicted_tag[0]-1]\n",
    "    # print(f': {most_likely_answer.words}\\n')\n",
    "    # print(f'Predicted Answer: {most_likely_answer}\\n')\n",
    "\n",
    "    id_ = example[\"id\"]\n",
    "    predictions.append(\n",
    "        {'id': id_,\n",
    "         'prediction_text':\n",
    "             most_likely_answer,\n",
    "         'no_answer_probability': 0.0\n",
    "         }\n",
    "    )\n",
    "\n",
    "    #just using their answers\n",
    "    references.append(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"answers\": example[\"answers\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "predictions[:5]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question in train dataset 20431\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'exact': 0.8467524839704371,\n 'f1': 17.2646973229193,\n 'total': 20431,\n 'HasAns_exact': 0.8467524839704371,\n 'HasAns_f1': 17.2646973229193,\n 'HasAns_total': 20431,\n 'best_exact': 0.8467524839704371,\n 'best_exact_thresh': 0.0,\n 'best_f1': 17.2646973229193,\n 'best_f1_thresh': 0.0}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "print(f'Number of question in train dataset {rc_dataset.shape[0]}')\n",
    "metric = load_metric(\"squad_v2\")\n",
    "metric.add_batch(predictions=predictions, references=references)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does this compare to guessing at random?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': '9f44c1539efe6f7e79b02eb1b413aa43_1',\n  'prediction_text': 'By statute , you must report a change of address to DMV within ten days of moving. That is the case for the address associated with your license, as well as all the addresses associated with each registered vehicle, which may differ. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_3',\n  'prediction_text': 'The $300 DRA fee can be paid in $100 annual installments over three years. Motorists who fail to maintain an updated address with DMV may resolve their tickets with the court, but never receive their DRA assessment because we do not have their new address on record. Failure to pay the DRA will result in a suspended license. ',\n  'no_answer_probability': 0.0},\n {'id': '9f44c1539efe6f7e79b02eb1b413aa43_5',\n  'prediction_text': '\\n\\n5. Not Bringing Proper Documentation to DMV Office \\n',\n  'no_answer_probability': 0.0}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_predictions = []\n",
    "random_references = []\n",
    "for example in rc_dataset:\n",
    "    question_ = example[\"question\"]\n",
    "    doc_id =example['title']\n",
    "\n",
    "    #pick a random text from the document\n",
    "    texts = raw_training_text_per_doc[doc_id]\n",
    "    most_likely_answer = texts[random.randint(0, len(texts)-1)]\n",
    "\n",
    "    id_ = example[\"id\"]\n",
    "    random_predictions.append(\n",
    "        {'id': id_,\n",
    "         'prediction_text':\n",
    "             most_likely_answer,\n",
    "         'no_answer_probability': 0.0\n",
    "         }\n",
    "    )\n",
    "\n",
    "    #just using their answers\n",
    "    random_references.append(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"answers\": example[\"answers\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "random_predictions[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for random text predictions for each question\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'exact': 1.5075130928491018,\n 'f1': 14.794680360709638,\n 'total': 20431,\n 'HasAns_exact': 1.5075130928491018,\n 'HasAns_f1': 14.794680360709638,\n 'HasAns_total': 20431,\n 'best_exact': 1.5075130928491018,\n 'best_exact_thresh': 0.0,\n 'best_f1': 14.794680360709638,\n 'best_f1_thresh': 0.0}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Results for random text predictions for each question')\n",
    "metric = load_metric(\"squad_v2\")\n",
    "metric.add_batch(predictions=random_predictions, references=random_references)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for validation part of rc dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rc_validation_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"doc2dial_rc\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question in train dataset 3972\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'About New York State Inspections#3_0'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/tb/kxdncxw16gx7p6y4hsgc0y540000gn/T/ipykernel_41033/723857626.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mtest_doc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msimple_preprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeacc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0;31m#calculate vector using model for that document\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodels_for_doc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdoc_id\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     \u001B[0mvector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_doc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;31m#find the most similar document (spans)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'About New York State Inspections#3_0'"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "print(f'Number of question in train dataset {rc_validation_dataset.shape[0]}')\n",
    "\n",
    "for example in rc_validation_dataset:\n",
    "    question_ = example[\"question\"]\n",
    "    doc_id =example['title']\n",
    "    # it does better if the user and agent string are left\n",
    "    # question_ = question_.replace('user:', '')\n",
    "    # question_ = question_.replace('agent:', '')\n",
    "\n",
    "    #preprocess question in the same way\n",
    "    test_doc = simple_preprocess(question_, deacc=True)\n",
    "    #calculate vector using model for that document\n",
    "    model = models_for_doc[doc_id]\n",
    "    vector = model.infer_vector(test_doc)\n",
    "    #find the most similar document (spans)\n",
    "    sims = model.dv.most_similar([vector], topn=1)\n",
    "    most_likely_answer = raw_training_text_per_doc[doc_id][sims[0][0]]\n",
    "    # most_likely_answer = training_docs[sims[0][0] - 1]\n",
    "    # most_likely_predicted_tag = most_likely_answer.tags\n",
    "    #find original text for tag\n",
    "    # most_likely_predicted_text = spans[most_likely_predicted_tag[0]-1]\n",
    "    # print(f': {most_likely_answer.words}\\n')\n",
    "    # print(f'Predicted Answer: {most_likely_answer}\\n')\n",
    "\n",
    "    id_ = example[\"id\"]\n",
    "    predictions.append(\n",
    "        {'id': id_,\n",
    "         'prediction_text':\n",
    "             most_likely_answer,\n",
    "         'no_answer_probability': 0.0\n",
    "         }\n",
    "    )\n",
    "\n",
    "    #just using their answers\n",
    "    references.append(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"answers\": example[\"answers\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "predictions[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions:\n",
    "- Given that we don't have all the documents to train does that make sense for a Doc2Vec approach or does it make sense\n",
    "to create a model per document?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}